/*
 * This file was automatically generated by UTestGen and EvoSuite
 * Sun Jul 13 21:13:42 GMT 2025
 */

package opennlp.tools.tokenize;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.IOException;
import java.io.Reader;
import java.io.StringReader;
import java.io.Writer;
import java.util.HashMap;
import java.util.LinkedHashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.Stack;
import java.util.regex.Pattern;
import opennlp.tools.dictionary.Dictionary;
import opennlp.tools.ml.model.MaxentModel;
import opennlp.tools.parser.lang.en.HeadRules;
import opennlp.tools.tokenize.DetokenizationDictionary;
import opennlp.tools.tokenize.DictionaryDetokenizer;
import opennlp.tools.tokenize.TokenContextGenerator;
import opennlp.tools.tokenize.TokenSample;
import opennlp.tools.tokenize.TokenizerFactory;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.tokenize.lang.Factory;
import opennlp.tools.util.CollectionObjectStream;
import opennlp.tools.util.ObjectStream;
import opennlp.tools.util.Span;
import opennlp.tools.util.StringList;
import opennlp.tools.util.TrainingParameters;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, separateClassLoader = true) 
public class TokenizerME_3_ESTest extends TokenizerME_3_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void testTrainThrowsIOExceptionAndCreatesTokenizerMETaking2ArgumentsAndTokenizePos()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[4] = 0.0;
      doubleArray0[7] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("_bv|?9?N]3", "T", "_bv|?9?N]3", "_bv|?9?N]3", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(2, 2, 2, 2, 2).when(maxentModel0).getIndex(anyString());
      Pattern.compile("T");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(false);
      StringList stringList0 = new StringList("_bv|?9?N]3");
      StringList stringList1 = stringList0.toCaseSensitive();
      StringList stringList2 = stringList1.toCaseInsensitive();
      dictionary0.put(stringList2);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.keepNewLines = false;
      tokenizerME0.tokenizePos("_bv|?9?N]3");
      LinkedHashSet<TokenSample> linkedHashSet0 = new LinkedHashSet<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedHashSet0);
      TrainingParameters trainingParameters0 = TrainingParameters.defaultParams();
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationReturningTrue()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("J?6w\"t^JGPV.i1s5P").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      StringList stringList0 = new StringList("J?6w\"t^JGPV.i1s5P");
      dictionary0.put(stringList0);
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.isAcceptableAbbreviation("J?6w\"t^JGPV.i1s5P");
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("J?6w\"t^JGPV.i1s5P");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndCreatesTokenizerMETaking2Arguments0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("useAlphaNumericOptimization is a mandatory property!").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("useAlphaNumericOptimization is a mandatory property!");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndTokenizePosAndCreatesTokenizerMETaking2Arguments()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      tokenizerFactory0.createManifestEntries();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("Z<h}.").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      System.setCurrentTimeMillis(2);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("L0IaF!K; ]{x?4\u0000I");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndCreatesTokenizerMETaking2Arguments1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("yDN_`CzR:w").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("yDN_`CzR:w");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingStringThrowsIOException()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      String string0 = "[a-zA-Z]+";
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      String string1 = "[a-zA-Z]+";
      Pattern.compile("F");
      Dictionary dictionary1 = new Dictionary(false);
      TokenizerModel tokenizerModel1 = null;
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("F");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndTokenizePos0()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 1.0E-4;
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (-2970.858247);
      doubleArray0[5] = 0.1;
      doubleArray0[6] = 1.0;
      doubleArray0[7] = 1154.136944;
      double[] doubleArray1 = new double[0];
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      String[] stringArray0 = new String[0];
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      doReturn((String[]) null).when(tokenContextGenerator0).getContext(anyString() , anyInt());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Dictionary dictionary1 = new Dictionary(true);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos0()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[7] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("_bv|?9?N]3", "T", "_bv|?9?N]3", "_bv|?9?N]3", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(2, 2, 2, 2, 2).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("T").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("_bv|?9?N]3");
      assertEquals(7, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimizationReturningTrue()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      tokenizerFactory0.createManifestEntries();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("i\" }Ie!4-.E-").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      System.setCurrentTimeMillis(2);
      boolean boolean0 = tokenizerME0.useAlphaNumericOptimization();
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2Arguments0()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      tokenizerFactory0.createManifestEntries();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2Arguments1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilities()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("F");
      assertEquals(1, spanArray0.length);
      
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {1.0}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesReturningNonEmptyArray()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      int int0 = 2;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("T");
      Dictionary dictionary1 = new Dictionary(true);
      tokenizerME0.getTokenProbabilities();
      tokenizerME0.useAlphaNumericOptimization();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModelAndGetTokenProbabilities()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesReturningEmptyArray()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Dictionary dictionary1 = new Dictionary(true);
      tokenizerME0.getTokenProbabilities();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndIsAcceptableAbbreviation()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.isAcceptableAbbreviation("Tg3#[;oC88pr^n~A0qA");
      Dictionary dictionary1 = new Dictionary(true);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("i\" }Ie!4-.E-");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndIsAcceptableAbbreviation()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("J?6w\"t^JGPV.i1s5P").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation("J?6w\"t^JGPV.i1s5P");
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviation()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsArrayIndexOutOfBoundsException()  throws Throwable  {
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = 1188.6;
      doubleArray0[1] = (-4048.3972949);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn(doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("yxOy1E01}5&+{").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn((-565)).when(maxentModel0).getIndex(anyString());
      String[] stringArray0 = new String[5];
      stringArray0[0] = "yxOy1E01}5&+{";
      stringArray0[1] = "yxOy1E01}5&+{";
      stringArray0[2] = "yxOy1E01}5&+{";
      stringArray0[3] = "yxOy1E01}5&+{";
      stringArray0[4] = "yxOy1E01}5&+{";
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      doReturn(stringArray0).when(tokenContextGenerator0).getContext(anyString() , anyInt());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      pattern0.splitAsStream("yxOy1E01}5&+{");
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Dictionary dictionary1 = new Dictionary(true);
      tokenizerME0.tokenizePos("T");
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // Index -565 out of bounds for length 2
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndCreatesTokenizerMETakingTokenizerModel0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("a 8\"1^AG1~MLQdK3C");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimization()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[0] = (-2970.858247);
      doubleArray0[5] = (-2970.858247);
      doubleArray0[6] = 1.0;
      doubleArray0[7] = (-2970.858247);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("Tg3#[;oC88pr^n~A0qA", "", "zmTBWJrV0zY1E", (String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(2, 2, 2, 0).when(maxentModel0).getIndex(anyString());
      String[] stringArray0 = new String[0];
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(true);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Dictionary dictionary1 = new Dictionary(false);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndTokenizePosReturningNonEmptyArray()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 1.0E-4;
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (-2970.858247);
      doubleArray0[5] = 0.1;
      doubleArray0[6] = 1.0;
      doubleArray0[7] = 1154.136944;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      String[] stringArray0 = new String[0];
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      doReturn((String[]) null).when(tokenContextGenerator0).getContext(anyString() , anyInt());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Dictionary dictionary1 = new Dictionary(true);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsIOExceptionAndTokenizePos()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 0.0;
      doubleArray0[7] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("_bv|?9?N]3", "T", "_bv|?9?N]3", "_bv|?9?N]3", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(2, 2, 2, 2, 2).when(maxentModel0).getIndex(anyString());
      Pattern.compile("T");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary(false);
      StringList stringList0 = new StringList("_bv|?9?N]3");
      StringList stringList1 = stringList0.toCaseSensitive();
      StringList stringList2 = stringList1.toCaseInsensitive();
      dictionary0.put(stringList2);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.keepNewLines = false;
      tokenizerME0.tokenizePos("_bv|?9?N]3");
      LinkedHashSet<TokenSample> linkedHashSet0 = new LinkedHashSet<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedHashSet0);
      TrainingParameters trainingParameters0 = TrainingParameters.defaultParams();
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos1()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("_bv|?9?N]3", "_bv|?9?N]3", "_bv|?9?N]3", "_bv|?9?N]3", "_bv|?9?N]3").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(2, 2, 2, 2, 2).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("_bv|?9?N]3").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("_bv|?9?N]3");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndTokenizePos1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      doReturn((String[]) null).when(tokenContextGenerator0).getContext(anyString() , anyInt());
      String string0 = "[a-zA-Z]+";
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      boolean boolean0 = false;
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      String string1 = "}Wg<jZ6HFnC-B/e";
      boolean boolean1 = true;
      Dictionary dictionary1 = new Dictionary(true);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndCreatesTokenizerMETakingTokenizerModel1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("N739L'\"C`x5nn%R");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePos()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[3] = 1.0E-4;
      doubleArray0[2] = 540.1364013;
      doubleArray0[3] = 0.0;
      doubleArray0[6] = 1.0;
      doubleArray0[7] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      tokenizerFactory0.getAlphaNumericPattern();
      Dictionary dictionary0 = new Dictionary(true);
      tokenizerFactory0.createManifestEntries();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("i\" }Ie!4-.E-").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertTrue(tokenizerME0.useAlphaNumericOptimization());
      
      Span[] spanArray0 = tokenizerME0.tokenizePos("cc");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testTokenizePosReturningEmptyArray()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      Dictionary dictionary0 = new Dictionary(false);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      String string0 = "";
      String string1 = "}Wg<jZ6HFnC-B/e";
      boolean boolean0 = true;
      Dictionary dictionary1 = new Dictionary(true);
      tokenizerME0.tokenizePos("");
      Stack<TokenSample> stack0 = new Stack<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(stack0);
      StringReader stringReader0 = new StringReader("");
      Dictionary.parseOneEntryPerLine(stringReader0);
      TokenizerFactory tokenizerFactory1 = new TokenizerFactory("prefix must not be null", dictionary1, false, pattern0);
      TrainingParameters trainingParameters0 = TrainingParameters.defaultParams();
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory1, trainingParameters0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimizationThrowsIOException()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      LinkedList<TokenSample> linkedList0 = new LinkedList<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedList0);
      Dictionary dictionary0 = new Dictionary(true);
      try { 
        TokenizerFactory.create("", "}Wg<jZ6HFnC-B/e", dictionary0, true, pattern0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Could not instantiate the . The initialization throw an exception.
         //
         verifyException("opennlp.tools.tokenize.TokenizerFactory", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((TokenizerFactory) null).when(tokenizerModel0).getFactory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME(tokenizerModel0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerFactory.getAlphaNumericPattern()\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException()  throws Throwable  {
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null, (Factory) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException()  throws Throwable  {
      Factory factory0 = new Factory();
      Reader reader0 = Reader.nullReader();
      HeadRules headRules0 = new HeadRules(reader0);
      Writer writer0 = Writer.nullWriter();
      headRules0.serialize(writer0);
      Set<String> set0 = headRules0.getPunctuationTags();
      factory0.createTokenContextGenerator("", set0);
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsIOException()  throws Throwable  {
      DictionaryDetokenizer dictionaryDetokenizer0 = new DictionaryDetokenizer((DetokenizationDictionary) null);
      String[] stringArray0 = new String[0];
      TokenSample tokenSample0 = new TokenSample(dictionaryDetokenizer0, stringArray0);
      List<TokenSample> list0 = List.of(tokenSample0, tokenSample0, tokenSample0, tokenSample0, tokenSample0);
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(list0);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TrainingParameters trainingParameters0 = TrainingParameters.setParams(stringArray0);
      Map<String, Object> map0 = trainingParameters0.getObjectSettings("F");
      TrainingParameters trainingParameters1 = new TrainingParameters(map0);
      TrainingParameters trainingParameters2 = new TrainingParameters(trainingParameters1);
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory0, trainingParameters2);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException1()  throws Throwable  {
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getFactory()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsNullPointerExceptionAndTrain()  throws Throwable  {
      LinkedList<TokenSample> linkedList0 = new LinkedList<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedList0);
      HashMap<String, Object> hashMap0 = new HashMap<String, Object>();
      Object object0 = new Object();
      List.of(object0);
      TrainingParameters trainingParameters0 = new TrainingParameters(hashMap0);
      // Undeclared exception!
      try { 
        TokenizerME.train(collectionObjectStream0, (TokenizerFactory) null, trainingParameters0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerFactory.isUseAlphaNumericOptimization()\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsNullPointerExceptionAndTrainWithNullAndTrainWithNonNull()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TrainingParameters trainingParameters0 = new TrainingParameters();
      // Undeclared exception!
      try { 
        TokenizerME.train((ObjectStream<TokenSample>) null, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.util.ObjectStream.read()\" because \"this.samples\" is null
         //
         verifyException("opennlp.tools.util.AbstractEventStream", e);
      }
  }
}
