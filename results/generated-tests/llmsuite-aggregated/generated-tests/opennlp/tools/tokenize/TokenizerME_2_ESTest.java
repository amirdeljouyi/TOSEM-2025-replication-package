/*
 * This file was automatically generated by UTestGen and EvoSuite
 * Sun Jul 13 21:13:10 GMT 2025
 */

package opennlp.tools.tokenize;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.IOException;
import java.nio.CharBuffer;
import java.util.Map;
import java.util.Stack;
import java.util.regex.Pattern;
import opennlp.tools.dictionary.Dictionary;
import opennlp.tools.ml.model.MaxentModel;
import opennlp.tools.postag.POSSample;
import opennlp.tools.postag.POSTaggerME;
import opennlp.tools.tokenize.TokenContextGenerator;
import opennlp.tools.tokenize.TokenSample;
import opennlp.tools.tokenize.TokenizerFactory;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.tokenize.lang.Factory;
import opennlp.tools.util.CollectionObjectStream;
import opennlp.tools.util.ObjectStream;
import opennlp.tools.util.Span;
import opennlp.tools.util.StringList;
import opennlp.tools.util.TrainingParameters;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, separateClassLoader = true) 
public class TokenizerME_2_ESTest extends TokenizerME_2_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationReturningTrue()  throws Throwable  {
      double[] doubleArray0 = new double[7];
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "Downloading model from {} to {}.");
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("m", "m", "m", "T", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("generamor#A0").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      StringList stringList0 = new StringList("generamor#A0");
      dictionary0.put(stringList0);
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("generamor#A0");
      assertEquals(8, spanArray0.length);
      
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation("generamor#A0");
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos0()  throws Throwable  {
      double[] doubleArray0 = new double[7];
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "m");
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("m", "m", "m", "T", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("generamor#A0").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      StringList stringList0 = new StringList("generamor#A0");
      dictionary0.put(stringList0);
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("generamor#A0");
      assertEquals(8, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndGetTokenProbabilities()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.getTokenProbabilities();
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingStringThrowsIOException()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("n@Wk~6k+q2'R)`A");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("n@W4k~q6r+N:id.`A").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertTrue(tokenizerME0.useAlphaNumericOptimization());
      
      Span[] spanArray0 = tokenizerME0.tokenizePos("TT");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimizationReturningFalse()  throws Throwable  {
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = (-1.0);
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 0.0;
      doubleArray0[5] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.tokenizePos("");
      tokenizerME0.useAlphaNumericOptimization();
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("MERGE_TO_RIGHT");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingString()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 1.0;
      doubleArray0[4] = 1678.5747;
      doubleArray0[6] = (-100000.0);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("", "F", "F", "T", "a^v<5{PA7?").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      String string0 = "[a-zA-Z]+";
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      String string1 = "generator#0";
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, (-2));
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.tokenizePos("po=vz*");
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("a^v<5{PA7?");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2Arguments()  throws Throwable  {
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationAndCreatesTokenizerMETaking2Arguments()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("MERGE_TO_RIGHT").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
      
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation("MERGE_TO_RIGHT");
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testTokenizePosReturningEmptyArray()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 1678.5747;
      doubleArray0[6] = 1678.5747;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("");
      boolean boolean0 = tokenizerME0.useAlphaNumericOptimization();
      assertTrue(boolean0);
      
      tokenizerME0.getTokenProbabilities();
      assertTrue(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException0()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 1678.5747;
      doubleArray0[6] = 1678.5747;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME(tokenizerModel0, (Factory) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.lang.Factory.getAlphanumeric(String)\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimization()  throws Throwable  {
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 1.0E-7;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = (-172.23);
      doubleArray0[3] = (-1.0);
      doubleArray0[4] = 0.0;
      doubleArray0[5] = 0.0;
      doubleArray0[6] = (-2076.4503790397475);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("|5P", "", "", "", "MERGE_TO_RIGHT").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("NboV^`Dt");
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      tokenizerME0.keepNewLines = false;
      double[] doubleArray1 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {9.999999999999996E-50}, doubleArray1, 0.01);
      
      tokenizerME0.isAcceptableAbbreviation("NboV^`Dt");
      boolean boolean0 = tokenizerME0.useAlphaNumericOptimization();
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingStringThrowsIOExceptionAndTokenizePos()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 1.0;
      doubleArray0[4] = 1678.5747;
      doubleArray0[6] = (-100000.0);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("", "F", "F", "T", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, (-2));
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("generator#0").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      tokenizerME0.tokenizePos("generator#0");
      tokenizerME0.tokenizePos("Z2");
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("po=vz*");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos2()  throws Throwable  {
      double[] doubleArray0 = new double[7];
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("m", "m", "m", "T", "T").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("generator#0").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("generator#0");
      assertEquals(8, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testTokenizePos()  throws Throwable  {
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = (-1350.1150205);
      doubleArray0[2] = 1.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 300.1147;
      doubleArray0[5] = (-1375.44400201);
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      String string0 = "[a-zA-Z]+";
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerFactory tokenizerFactory2 = new TokenizerFactory("T", dictionary0, true, pattern0);
      tokenizerFactory2.getContextGenerator();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("T");
      char[] charArray0 = new char[2];
      charArray0[0] = '.';
      charArray0[1] = 'U';
      // Undeclared exception!
      try { 
        CharBuffer.wrap(charArray0, 2, 2);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.nio.CharBuffer", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos3()  throws Throwable  {
      double[] doubleArray0 = new double[7];
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0, (Object) doubleArray0).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn("m", "m", "m", "m", "m").when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0, 0, 0, 0, 0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("generamor#A0").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("generamor#A0");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesAndCreatesTokenizerMETaking2ArgumentsAndTokenizePos()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("T").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("T");
      assertEquals(1, spanArray0.length);
      
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {1.0}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesReturningNonEmptyArray()  throws Throwable  {
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (-890.0860740282);
      doubleArray0[1] = 1.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 1678.5747;
      doubleArray0[5] = (-2395.794);
      doubleArray0[6] = (-100000.0);
      doubleArray0[7] = 767.136;
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("NboV^`Dt");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationAndFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Dictionary dictionary0 = new Dictionary(true);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("A", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      Factory factory0 = new Factory();
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesAndIsAcceptableAbbreviation()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertEquals(0, doubleArray0.length);
      
      tokenizerME0.getTokenProbabilities();
      System.setCurrentTimeMillis(874L);
      CharBuffer charBuffer0 = CharBuffer.wrap((CharSequence) "[a-zA-Z]+");
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation(charBuffer0);
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationWithNonNull()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.getTokenProbabilities();
      System.setCurrentTimeMillis(874L);
      CharBuffer charBuffer0 = CharBuffer.wrap((CharSequence) "[a-zA-Z]+");
      boolean boolean0 = tokenizerME0.isAcceptableAbbreviation(charBuffer0);
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndTokenizePos()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      String string0 = "[a-zA-Z]+";
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Stack<POSSample> stack0 = new Stack<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(stack0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 2);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory("generator#0", dictionary0, true, pattern0);
      TokenContextGenerator tokenContextGenerator0 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory1).getContextGenerator();
      doReturn(true).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      String string1 = "NboV^`Dt";
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("NboV^`Dt");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerException0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("MERGE_TO_RIGHT").when(tokenizerModel0).getLanguage();
      Dictionary dictionary1 = new Dictionary();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("MERGE_TO_RIGHT");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationThrowsNullPointerException()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      String string0 = "[a-zA-Z]+";
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("NboV^`Dt");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenContextGenerator.getContext(String, int)\" because \"this.cg\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerException1()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      doReturn((double[]) null).when(maxentModel0).eval(any(java.lang.String[].class));
      doReturn((String) null).when(maxentModel0).getBestOutcome(any(double[].class));
      doReturn(0).when(maxentModel0).getIndex(anyString());
      Dictionary dictionary0 = new Dictionary();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn("n@W4k~q6r+N:id.`A").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("n@W4k~q6r+N:id.`A");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot load from double array because \"probs\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModel()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((TokenizerFactory) null).when(tokenizerModel0).getFactory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME(tokenizerModel0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerFactory.getAlphaNumericPattern()\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException1()  throws Throwable  {
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException1()  throws Throwable  {
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getFactory()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsIOException()  throws Throwable  {
      Stack<TokenSample> stack0 = new Stack<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(stack0);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Map<String, Object> map0 = tokenizerFactory0.createArtifactMap();
      TrainingParameters trainingParameters0 = new TrainingParameters(map0);
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsNullPointerException()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TrainingParameters trainingParameters0 = new TrainingParameters();
      // Undeclared exception!
      try { 
        TokenizerME.train((ObjectStream<TokenSample>) null, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.util.ObjectStream.read()\" because \"this.samples\" is null
         //
         verifyException("opennlp.tools.util.AbstractEventStream", e);
      }
  }
}
