/*
 * This file was automatically generated by UTestGen and EvoSuite
 * Sun Jul 13 21:26:47 GMT 2025
 */

package opennlp.tools.tokenize;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.IOException;
import java.io.PipedInputStream;
import java.io.PipedOutputStream;
import java.io.Reader;
import java.io.StringReader;
import java.util.LinkedList;
import java.util.regex.Pattern;
import opennlp.tools.dictionary.Dictionary;
import opennlp.tools.ml.maxent.GISModel;
import opennlp.tools.ml.model.Context;
import opennlp.tools.ml.model.MaxentModel;
import opennlp.tools.ml.model.UniformPrior;
import opennlp.tools.ml.perceptron.PerceptronModel;
import opennlp.tools.postag.POSSample;
import opennlp.tools.postag.POSTaggerME;
import opennlp.tools.tokenize.TokenContextGenerator;
import opennlp.tools.tokenize.TokenSample;
import opennlp.tools.tokenize.TokenizerFactory;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
import opennlp.tools.tokenize.lang.Factory;
import opennlp.tools.util.CollectionObjectStream;
import opennlp.tools.util.ObjectStream;
import opennlp.tools.util.Span;
import opennlp.tools.util.TrainingParameters;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, separateClassLoader = true) 
public class TokenizerME_5_ESTest extends TokenizerME_5_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesAndTokenizePos()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[21];
      String[] stringArray0 = new String[2];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("T");
      assertEquals(1, spanArray0.length);
      
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {1.0}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesReturningNonEmptyArray()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Factory.DEFAULT_ALPHANUMERIC;
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = new TokenizerFactory();
      String string0 = "0";
      StringReader stringReader0 = new StringReader("0");
      StringReader stringReader1 = new StringReader("0");
      Dictionary dictionary0 = Dictionary.parseOneEntryPerLine(stringReader1);
      Context[] contextArray0 = new Context[8];
      String[] stringArray0 = new String[6];
      stringArray0[0] = "0";
      stringArray0[4] = "0";
      stringArray0[5] = "0";
      UniformPrior uniformPrior0 = new UniformPrior();
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0, uniformPrior0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("-RRB-");
      tokenizerME0.getTokenProbabilities();
      tokenizerME0.tokenizePos("wa");
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("0");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimizationReturningTrue()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      Pattern.matches("", "");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(true).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(maxentModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.getTokenProbabilities();
      System.setCurrentTimeMillis(1L);
      boolean boolean0 = tokenizerME0.useAlphaNumericOptimization();
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void testTokenizePos0()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      Pattern pattern0 = Factory.DEFAULT_ALPHANUMERIC;
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TokenContextGenerator tokenContextGenerator1 = tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory1).getAlphaNumericPattern();
      doReturn(tokenContextGenerator1).when(tokenizerFactory1).getContextGenerator();
      doReturn(false).when(tokenizerFactory1).isUseAlphaNumericOptimization();
      StringReader stringReader0 = new StringReader("");
      Dictionary dictionary0 = Dictionary.parseOneEntryPerLine(stringReader0);
      Context[] contextArray0 = new Context[8];
      int[] intArray0 = new int[9];
      intArray0[0] = 2655;
      intArray0[1] = 2130;
      intArray0[2] = 290;
      intArray0[3] = 2147483645;
      intArray0[4] = 50;
      intArray0[5] = (-860);
      intArray0[6] = 3;
      intArray0[7] = 10;
      intArray0[8] = (-1);
      Context context0 = new Context(intArray0, (double[]) null);
      contextArray0[0] = context0;
      Context context1 = new Context(intArray0, (double[]) null);
      contextArray0[2] = context1;
      String[] stringArray0 = new String[8];
      stringArray0[0] = "";
      stringArray0[1] = "[a-zA-Z]+";
      stringArray0[2] = "[a-zA-Z]+";
      stringArray0[3] = "[a-zA-Z]+";
      stringArray0[4] = "";
      stringArray0[5] = "A";
      stringArray0[6] = "[a-zA-Z]+";
      stringArray0[7] = "[a-zA-Z]+";
      UniformPrior uniformPrior0 = new UniformPrior();
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0, uniformPrior0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("wa");
      tokenizerME0.tokenizePos("besA");
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "wa");
      tokenizerME0.isAcceptableAbbreviation("besA");
      tokenizerME0.tokenizePos("CHUNKER");
      System.setCurrentTimeMillis(0L);
      System.setCurrentTimeMillis((-598L));
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModelAndTokenizePos0()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[8];
      String[] stringArray0 = new String[8];
      stringArray0[0] = "T";
      UniformPrior uniformPrior0 = new UniformPrior();
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0, uniformPrior0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("Mp\u0004bYi*x:J_rcyz}c");
      assertEquals(18, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModelAndTokenizePos1()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[8];
      String[] stringArray0 = new String[8];
      stringArray0[0] = "";
      UniformPrior uniformPrior0 = new UniformPrior();
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0, uniformPrior0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      Span[] spanArray0 = tokenizerME0.tokenizePos("Mp\u0004bYi*:J_rcyz}c");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingStringThrowsIOException()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = new TokenizerFactory();
      String string0 = "T";
      StringReader stringReader0 = new StringReader("T");
      StringReader stringReader1 = new StringReader("T");
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[8];
      String[] stringArray0 = new String[8];
      stringArray0[0] = "T";
      stringArray0[4] = "T";
      stringArray0[5] = "T";
      UniformPrior uniformPrior0 = new UniformPrior();
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0, uniformPrior0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory1).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("wa");
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("T");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testUseAlphaNumericOptimization()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("");
      tokenizerME0.getTokenProbabilities();
      System.setCurrentTimeMillis(1L);
      boolean boolean0 = tokenizerME0.useAlphaNumericOptimization();
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModelAndGetTokenProbabilities()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[10];
      String[] stringArray0 = new String[1];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      double[] doubleArray0 = tokenizerME0.getTokenProbabilities();
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
  }

  @Test(timeout = 4000)
  public void testGetTokenProbabilitiesAndGetTokenProbabilities()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.getTokenProbabilities();
      System.setCurrentTimeMillis(1L);
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2Arguments0()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      tokenizerFactory0.getContextGenerator();
      TokenizerFactory tokenizerFactory1 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      Dictionary dictionary0 = new Dictionary(true);
      Context[] contextArray0 = new Context[6];
      int[] intArray0 = new int[7];
      intArray0[0] = (-998);
      intArray0[1] = (-3259);
      intArray0[2] = 1464;
      intArray0[3] = 955;
      intArray0[4] = (-4014);
      intArray0[5] = (-304);
      intArray0[6] = (-2355);
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 955;
      doubleArray0[1] = (double) (-3259);
      doubleArray0[2] = (double) (-998);
      doubleArray0[3] = (double) (-998);
      doubleArray0[4] = (double) 1464;
      doubleArray0[5] = (double) (-304);
      doubleArray0[6] = (double) (-304);
      Context context0 = new Context(intArray0, doubleArray0);
      contextArray0[0] = context0;
      Context context1 = new Context(intArray0, doubleArray0);
      contextArray0[1] = context1;
      double[] doubleArray1 = new double[9];
      doubleArray1[0] = (double) (-4014);
      doubleArray1[1] = (double) (-4014);
      doubleArray1[2] = (double) (-3259);
      doubleArray1[3] = (double) (-4014);
      doubleArray1[4] = (double) (-998);
      doubleArray1[5] = (double) (-998);
      doubleArray1[6] = (double) (-998);
      doubleArray1[7] = (double) (-998);
      doubleArray1[8] = (double) (-304);
      Context context2 = new Context(intArray0, doubleArray1);
      contextArray0[2] = context2;
      Context context3 = new Context(intArray0, doubleArray0);
      contextArray0[3] = context3;
      int[] intArray1 = new int[7];
      intArray1[0] = (-998);
      intArray1[1] = (-998);
      intArray1[2] = (-4014);
      intArray1[3] = (-2355);
      intArray1[4] = (-304);
      intArray1[5] = 1464;
      intArray1[6] = 955;
      Context context4 = new Context(intArray1, doubleArray1);
      contextArray0[4] = context4;
      Context context5 = new Context(intArray0, doubleArray1);
      contextArray0[5] = context5;
      String[] stringArray0 = new String[0];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2Arguments1()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[13];
      String[] stringArray0 = new String[13];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, (Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(false).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn((String) null).when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertFalse(tokenizerME0.useAlphaNumericOptimization());
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsIOException()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      String string0 = "[a-zA-Z]+";
      Pattern.compile("[a-zA-Z]+");
      Pattern pattern0 = Factory.DEFAULT_ALPHANUMERIC;
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      Reader reader0 = Reader.nullReader();
      Dictionary dictionary0 = Dictionary.parseOneEntryPerLine(reader0);
      Context[] contextArray0 = new Context[4];
      Context context0 = mock(Context.class, new ViolatedAssumptionAnswer());
      contextArray0[0] = context0;
      contextArray0[1] = context0;
      contextArray0[2] = context0;
      contextArray0[3] = context0;
      String[] stringArray0 = new String[0];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("besA");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndTokenizePos()  throws Throwable  {
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[18];
      String[] stringArray0 = new String[0];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn(",[-zA-]+").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      assertTrue(tokenizerME0.useAlphaNumericOptimization());
      
      Span[] spanArray0 = tokenizerME0.tokenizePos("uQq8ThCKBDh");
      assertEquals(1, spanArray0.length);
  }

  @Test(timeout = 4000)
  public void testTokenizePosReturningEmptyArray()  throws Throwable  {
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      LinkedList<POSSample> linkedList0 = new LinkedList<POSSample>();
      CollectionObjectStream<POSSample> collectionObjectStream0 = new CollectionObjectStream<POSSample>(linkedList0);
      Dictionary dictionary0 = POSTaggerME.buildNGramDictionary(collectionObjectStream0, 50);
      Context[] contextArray0 = new Context[6];
      Context context0 = mock(Context.class, new ViolatedAssumptionAnswer());
      contextArray0[0] = context0;
      contextArray0[1] = context0;
      contextArray0[2] = context0;
      contextArray0[3] = context0;
      contextArray0[4] = context0;
      contextArray0[5] = context0;
      String[] stringArray0 = new String[0];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("");
      tokenizerME0.tokenizePos("F");
      tokenizerME0.isAcceptableAbbreviation("");
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos((String) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"String.length()\" because \"d\" is null
         //
         verifyException("opennlp.tools.tokenize.WhitespaceTokenizer", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationReturningTrue()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      doReturn((String[]) null).when(tokenContextGenerator0).getContext(anyString() , anyInt());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(true).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      StringReader stringReader0 = new StringReader("[a-zA-Z]+");
      stringReader0.mark(992);
      Dictionary dictionary0 = Dictionary.parseOneEntryPerLine(stringReader0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.tokenizePos("besA");
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.ml.model.MaxentModel.eval(String[])\" because \"this.model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsIllegalArgumentException()  throws Throwable  {
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[18];
      String[] stringArray0 = new String[0];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0, dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      doReturn(true).when(tokenizerModel0).useAlphaNumericOptimization();
      doReturn(",[-zA-]+").when(tokenizerModel0).getLanguage();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0, factory0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos(",[-zA-]+");
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Vector x is null or empty
         //
         verifyException("opennlp.tools.ml.ArrayMath", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviation()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      Dictionary dictionary0 = new Dictionary(false);
      Context[] contextArray0 = new Context[5];
      int[] intArray0 = new int[0];
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (-1186.04398903573);
      doubleArray0[1] = (-1591.3);
      doubleArray0[2] = (-926.8837754445939);
      Context context0 = new Context(intArray0, doubleArray0);
      contextArray0[0] = context0;
      Context context1 = new Context(intArray0, doubleArray0);
      contextArray0[1] = context1;
      Context context2 = new Context(intArray0, doubleArray0);
      contextArray0[2] = context2;
      Context context3 = new Context(intArray0, doubleArray0);
      contextArray0[3] = context3;
      Context context4 = new Context(intArray0, doubleArray0);
      contextArray0[4] = context4;
      String[] stringArray0 = new String[1];
      stringArray0[0] = "[a-zA-Z]+";
      PerceptronModel perceptronModel0 = new PerceptronModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(perceptronModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      PipedOutputStream pipedOutputStream0 = null;
      try {
        pipedOutputStream0 = new PipedOutputStream((PipedInputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.io.PipedOutputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void testIsAcceptableAbbreviationWithCharSequenceWhereLengthIsZero()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[15];
      String[] stringArray0 = new String[8];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      StringBuffer stringBuffer0 = new StringBuffer();
      tokenizerME0.isAcceptableAbbreviation(stringBuffer0);
      TokenizerME tokenizerME1 = null;
      try {
        tokenizerME1 = new TokenizerME("r.@(pF1)+`*\u0003{GrleD");
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Invalid model.
         //
         verifyException("opennlp.tools.util.DownloadUtil", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePos1()  throws Throwable  {
      String[] stringArray0 = new String[3];
      stringArray0[0] = "s;";
      stringArray0[1] = "l,In{Q9U@=0Oo&";
      stringArray0[2] = "7_1?BoQ";
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern pattern0 = Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn(pattern0).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn(tokenContextGenerator0).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      tokenizerME0.tokenizePos("w");
      System.setCurrentTimeMillis(0L);
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerException()  throws Throwable  {
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      Dictionary dictionary0 = new Dictionary();
      Context[] contextArray0 = new Context[22];
      String[] stringArray0 = new String[1];
      GISModel gISModel0 = new GISModel(contextArray0, stringArray0, stringArray0);
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn(dictionary0).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn(gISModel0).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("tokenSpans must not be null");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"String.equals(Object)\" because \"this.outcomeNames[i]\" is null
         //
         verifyException("opennlp.tools.ml.model.AbstractModel", e);
      }
  }

  @Test(timeout = 4000)
  public void testTokenizePosThrowsNullPointerExceptionAndIsAcceptableAbbreviationAndTokenizePos()  throws Throwable  {
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      tokenizerME0.isAcceptableAbbreviation("[a-zA-Z]+");
      // Undeclared exception!
      try { 
        tokenizerME0.tokenizePos("[a-zA-Z]+");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenContextGenerator.getContext(String, int)\" because \"this.cg\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETakingTokenizerModel()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      doReturn((Pattern) null).when(tokenizerFactory0).getAlphaNumericPattern();
      doReturn((TokenContextGenerator) null).when(tokenizerFactory0).getContextGenerator();
      doReturn(false).when(tokenizerFactory0).isUseAlphaNumericOptimization();
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((Dictionary) null).when(tokenizerModel0).getAbbreviations();
      doReturn(tokenizerFactory0).when(tokenizerModel0).getFactory();
      doReturn((MaxentModel) null).when(tokenizerModel0).getMaxentModel();
      TokenizerME tokenizerME0 = new TokenizerME(tokenizerModel0);
      System.setCurrentTimeMillis(788L);
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException0()  throws Throwable  {
      MaxentModel maxentModel0 = mock(MaxentModel.class, new ViolatedAssumptionAnswer());
      TokenContextGenerator tokenContextGenerator0 = mock(TokenContextGenerator.class, new ViolatedAssumptionAnswer());
      Pattern.compile("[a-zA-Z]+");
      TokenizerFactory tokenizerFactory0 = mock(TokenizerFactory.class, new ViolatedAssumptionAnswer());
      TokenizerModel tokenizerModel0 = mock(TokenizerModel.class, new ViolatedAssumptionAnswer());
      doReturn((TokenizerFactory) null).when(tokenizerModel0).getFactory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME(tokenizerModel0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerFactory.getAlphaNumericPattern()\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException()  throws Throwable  {
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testCreatesTokenizerMETaking2ArgumentsAndFailsToCreateTokenizerMETaking2ArgumentsThrowsNullPointerException()  throws Throwable  {
      LinkedList<TokenSample> linkedList0 = new LinkedList<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedList0);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TrainingParameters trainingParameters0 = new TrainingParameters();
      Factory factory0 = new Factory();
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null, factory0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getLanguage()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsIOException()  throws Throwable  {
      LinkedList<TokenSample> linkedList0 = new LinkedList<TokenSample>();
      CollectionObjectStream<TokenSample> collectionObjectStream0 = new CollectionObjectStream<TokenSample>(linkedList0);
      TokenizerFactory tokenizerFactory0 = new TokenizerFactory();
      TrainingParameters trainingParameters0 = new TrainingParameters();
      try { 
        TokenizerME.train(collectionObjectStream0, tokenizerFactory0, trainingParameters0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Insufficient training data to create model.
         //
         verifyException("opennlp.tools.ml.model.AbstractDataIndexer", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainThrowsNullPointerException()  throws Throwable  {
      // Undeclared exception!
      try { 
        TokenizerME.train((ObjectStream<TokenSample>) null, (TokenizerFactory) null, (TrainingParameters) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerFactory.isUseAlphaNumericOptimization()\" because \"factory\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }

  @Test(timeout = 4000)
  public void testFailsToCreateTokenizerMETakingTokenizerModelThrowsNullPointerException1()  throws Throwable  {
      TokenizerME tokenizerME0 = null;
      try {
        tokenizerME0 = new TokenizerME((TokenizerModel) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"opennlp.tools.tokenize.TokenizerModel.getFactory()\" because \"model\" is null
         //
         verifyException("opennlp.tools.tokenize.TokenizerME", e);
      }
  }
}
