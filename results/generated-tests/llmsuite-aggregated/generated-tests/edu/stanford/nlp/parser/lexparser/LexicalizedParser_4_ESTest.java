/*
 * This file was automatically generated by UTestGen and EvoSuite
 * Wed Apr 16 16:52:41 GMT 2025
 */

package edu.stanford.nlp.parser.lexparser;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import edu.stanford.nlp.ie.NERClassifierCombiner;
import edu.stanford.nlp.io.ExtensionFileFilter;
import edu.stanford.nlp.io.RegExFileFilter;
import edu.stanford.nlp.ling.CategoryWordTag;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.Label;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.ling.Word;
import edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams;
import edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams;
import edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor;
import edu.stanford.nlp.parser.lexparser.HebrewTreebankParserParams;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;
import edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams;
import edu.stanford.nlp.parser.lexparser.Options;
import edu.stanford.nlp.parser.lexparser.SpanishUnknownWordModelTrainer;
import edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer;
import edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions;
import edu.stanford.nlp.trees.Constituent;
import edu.stanford.nlp.trees.DiskTreebank;
import edu.stanford.nlp.trees.MemoryTreebank;
import edu.stanford.nlp.trees.PennTreebankLanguagePack;
import edu.stanford.nlp.trees.SemanticHeadFinder;
import edu.stanford.nlp.trees.SimpleConstituentFactory;
import edu.stanford.nlp.trees.TransformingTreebank;
import edu.stanford.nlp.trees.TreeGraphNode;
import edu.stanford.nlp.trees.TreeReaderFactory;
import edu.stanford.nlp.trees.Treebank;
import java.io.FileFilter;
import java.io.IOException;
import java.util.LinkedList;
import java.util.List;
import java.util.Properties;
import java.util.Stack;
import java.util.Vector;
import java.util.function.Function;
import java.util.regex.Pattern;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, separateClassLoader = true) 
public class LexicalizedParser_4_ESTest extends LexicalizedParser_4_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void testCopyLexicalizedParserReturningLexicalizedParserWhereRequiresTagsIsFalse()  throws Throwable  {
      Pattern pattern0 = Pattern.compile("");
      RegExFileFilter regExFileFilter0 = new RegExFileFilter(pattern0);
      pattern0.split((CharSequence) "", (-1224));
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
  }

  @Test(timeout = 4000)
  public void testBuildTrainBinarizerAndLoadModelTakingNoArguments()  throws Throwable  {
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      LexicalizedParser lexicalizedParser0 = LexicalizedParser.loadModel();
      assertNull(lexicalizedParser0);
  }

  @Test(timeout = 4000)
  public void testBuildTrainTransformerTakingOptions()  throws Throwable  {
      ExtensionFileFilter extensionFileFilter0 = new ExtensionFileFilter("z");
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel();
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainTransformer((Options) shiftReduceOptions0);
      Properties properties0 = new Properties();
      NERClassifierCombiner nERClassifierCombiner0 = null;
      try {
        nERClassifierCombiner0 = new NERClassifierCombiner(properties0);
        fail("Expecting exception: IOException");
      
      } catch(Throwable e) {
         //
         // Couldn't load classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz
         //
         verifyException("edu.stanford.nlp.ie.ClassifierCombiner", e);
      }
  }

  @Test(timeout = 4000)
  public void test()  throws Throwable  {
      NegraPennTreebankParserParams negraPennTreebankParserParams0 = new NegraPennTreebankParserParams();
      negraPennTreebankParserParams0.memoryTreebank();
      negraPennTreebankParserParams0.pw();
      Options options0 = new Options();
      ExactGrammarCompactor exactGrammarCompactor0 = new ExactGrammarCompactor(options0, false, false);
      Options.LexOptions options_LexOptions0 = new Options.LexOptions();
      options0.lexOptions = options_LexOptions0;
      String[] stringArray0 = new String[5];
      stringArray0[0] = "/u/scr/nlp/deeplearning/datasets/turian/embeddings-scaled.EMBEDDING_SIZE=25.txt";
      stringArray0[1] = null;
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams1 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
  }

  @Test(timeout = 4000)
  public void testTrainFromTreebankTaking3ArgumentsWithNullAndGetParserFromSerializedFile()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile("TAG_INDEX");
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      Options options0 = new Options(shiftReduceOptions0.tlpParams);
      options0.newTrainOptions();
      edu.stanford.nlp.parser.lexparser.LexicalizedParser lexicalizedParser0 = edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank("LK'I1l&_N 1", (FileFilter) null, options0);
      assertFalse(lexicalizedParser0.requiresTags());
  }

  @Test(timeout = 4000)
  public void testLoadModelTaking1And1AndBuildTrainBinarizer()  throws Throwable  {
      String[] stringArray0 = new String[5];
      stringArray0[0] = "";
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      Function.identity();
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      ExtensionFileFilter extensionFileFilter0 = new ExtensionFileFilter("edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory");
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      Options options0 = new Options(shiftReduceOptions0.tlpParams);
      String[] stringArray1 = new String[4];
      stringArray1[0] = null;
      stringArray1[1] = "";
      stringArray1[3] = "edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(options0, stringArray1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.parser.lexparser.LexicalizedParser.setOptionFlags(String[])\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testLoadModelTaking1And1WithNonEmptyStringAndNull()  throws Throwable  {
      ExtensionFileFilter extensionFileFilter0 = new ExtensionFileFilter("");
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte)52;
      byteArray0[1] = (byte) (-102);
      byteArray0[2] = (byte)52;
      byteArray0[3] = (byte)108;
      byteArray0[4] = (byte)101;
      byteArray0[5] = (byte)42;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel("done [read ", (List<String>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"java.util.List.size()\" because \"extraFlags\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testLoadModelTaking1And1AndLoadModelTaking1And1WithNonEmptyStringAndLoadModelTaking1And1WithNull()  throws Throwable  {
      NegraPennTreebankParserParams negraPennTreebankParserParams0 = new NegraPennTreebankParserParams();
      negraPennTreebankParserParams0.memoryTreebank();
      Options options0 = new Options();
      ExactGrammarCompactor exactGrammarCompactor0 = new ExactGrammarCompactor(options0, false, true);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      String[] stringArray0 = new String[9];
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte)15;
      byteArray0[1] = (byte)108;
      byteArray0[2] = (byte) (-27);
      byteArray0[3] = (byte)3;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      stringArray0[0] = "z(<~0@";
      stringArray0[1] = "1,:Aq\"";
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel("done [read ", (List<String>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"java.util.List.size()\" because \"extraFlags\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserQueryEvals()  throws Throwable  {
      String[] stringArray0 = new String[5];
      stringArray0[0] = "";
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
  }

  @Test(timeout = 4000)
  public void testMainWithNonEmptyArray()  throws Throwable  {
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot read field \"op\" because \"lp\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromTextFileReturningNull()  throws Throwable  {
      ExtensionFileFilter extensionFileFilter0 = new ExtensionFileFilter("useNegHKDict2");
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      PennTreebankLanguagePack pennTreebankLanguagePack0 = new PennTreebankLanguagePack();
      HebrewTreebankParserParams hebrewTreebankParserParams0 = new HebrewTreebankParserParams(pennTreebankLanguagePack0);
      hebrewTreebankParserParams0.headFinder();
  }

  @Test(timeout = 4000)
  public void testTrainFromTreebankTaking3ArgumentsWithNullAndBuildTrainBinarizerAndBuildTrainTransformerTaking2Arguments()  throws Throwable  {
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      TreeAnnotatorAndBinarizer treeAnnotatorAndBinarizer0 = edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      Function.identity();
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainTransformer((Options) shiftReduceOptions0, treeAnnotatorAndBinarizer0);
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      ExtensionFileFilter extensionFileFilter0 = new ExtensionFileFilter("n");
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      Options options0 = new Options(shiftReduceOptions0.tlpParams);
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank((String) null, (FileFilter) extensionFileFilter0, options0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.io.File", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromSerializedFile()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile("TAG_INDEX");
      TreeGraphNode treeGraphNode0 = null;
      try {
        treeGraphNode0 = new TreeGraphNode((Label) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.ling.Label.value()\" because \"oldLabel\" is null
         //
         verifyException("edu.stanford.nlp.ling.CoreLabel$CoreLabelFactory", e);
      }
  }

  @Test(timeout = 4000)
  public void testParserQueryThrowsNullPointerException()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.Options options0 = new edu.stanford.nlp.parser.lexparser.Options();
  }

  @Test(timeout = 4000)
  public void testLoadModelTaking1And1AndLoadModelTaking1And1WithNonEmptyStringAndLoadModelTaking1And1WithNonEmptyArray()  throws Throwable  {
      String[] stringArray0 = new String[2];
      stringArray0[0] = "]KcX'h4j;Mc'xpeiL";
      stringArray0[1] = "]KcX'h4j;Mc'xpeiL";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel("]KcX'h4j;Mc'xpeiL", stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.parser.lexparser.LexicalizedParser.setOptionFlags(String[])\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testTrainFromTreebankTaking2Arguments()  throws Throwable  {
      TransformingTreebank transformingTreebank0 = new TransformingTreebank();
      SpanishUnknownWordModelTrainer spanishUnknownWordModelTrainer0 = new SpanishUnknownWordModelTrainer();
      Options options0 = (Options)spanishUnknownWordModelTrainer0.op;
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.trainFromTreebank((Treebank) transformingTreebank0, (Options) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.parser.lexparser.Options.display()\" because \"op\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromSerializedFileThrowsNullPointerException()  throws Throwable  {
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      ChineseTreebankParserParams chineseTreebankParserParams0 = new ChineseTreebankParserParams();
      String[] stringArray0 = new String[8];
      stringArray0[0] = "word-processing";
      stringArray0[1] = null;
      stringArray0[2] = "word-processing";
      stringArray0[3] = null;
      stringArray0[4] = null;
      stringArray0[5] = "word-processing";
      stringArray0[6] = "edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory";
      stringArray0[7] = "edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot read field \"op\" because \"lp\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testMain()  throws Throwable  {
      String[] stringArray0 = new String[4];
      String string0 = "";
      stringArray0[0] = "";
      stringArray0[1] = "";
      String string1 = "";
      stringArray0[2] = "";
      stringArray0[3] = "Illegal access";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(stringArray0);
        fail("Expecting exception: StringIndexOutOfBoundsException");
      
      } catch(StringIndexOutOfBoundsException e) {
      }
  }

  @Test(timeout = 4000)
  public void testLoadModelTakingNoArgumentsThrowsStringIndexOutOfBoundsException()  throws Throwable  {
      String[] stringArray0 = new String[4];
      stringArray0[0] = "";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(stringArray0);
        fail("Expecting exception: StringIndexOutOfBoundsException");
      
      } catch(StringIndexOutOfBoundsException e) {
      }
  }

  @Test(timeout = 4000)
  public void testLoadModelTakingNoArguments()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.LexicalizedParser lexicalizedParser0 = edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel();
      assertNull(lexicalizedParser0);
      
      // Undeclared exception!
      try { 
        LexicalizedParser.copyLexicalizedParser((LexicalizedParser) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot read field \"lex\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testLoadModelTaking3ArgumentsWithNonEmptyStringAndNonNull()  throws Throwable  {
      String string0 = "TAG_INDEX";
      edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions shiftReduceOptions0 = new edu.stanford.nlp.parser.shiftreduce.ShiftReduceOptions();
      ExactGrammarCompactor exactGrammarCompactor0 = new ExactGrammarCompactor(shiftReduceOptions0, false, true);
      CategoryWordTag.factory();
      Options options0 = new Options(shiftReduceOptions0.tlpParams);
      edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory";
      // Undeclared exception!
      try { 
        edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel("TAG_INDEX", options0, stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.parser.lexparser.LexicalizedParser.setOptionFlags(String[])\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserQueryEvalsThrowsNullPointerException()  throws Throwable  {
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "";
      stringArray0[3] = "";
      stringArray0[4] = "";
      stringArray0[5] = "";
      stringArray0[6] = "";
      stringArray0[7] = ">-u^79W+H";
      stringArray0[8] = "";
      // Undeclared exception!
      try { 
        LexicalizedParser.loadModel("", (edu.stanford.nlp.parser.lexparser.Options) null, stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.parser.lexparser.LexicalizedParser.setOptionFlags(String[])\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromTreebankWithZeroAndNonEmptyList()  throws Throwable  {
      edu.stanford.nlp.io.ExtensionFileFilter extensionFileFilter0 = new edu.stanford.nlp.io.ExtensionFileFilter("");
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
      MemoryTreebank memoryTreebank0 = negraPennTreebankParserParams0.memoryTreebank();
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(shiftReduceOptions0, true, true);
      Stack<List<TaggedWord>> stack0 = new Stack<List<TaggedWord>>();
      Vector<TaggedWord> vector0 = new Vector<TaggedWord>();
      stack0.add((List<TaggedWord>) vector0);
      negraPennTreebankParserParams0.testMemoryTreebank();
      // Undeclared exception!
      LexicalizedParser.getParserFromTreebank(memoryTreebank0, memoryTreebank0, 0.0, exactGrammarCompactor0, shiftReduceOptions0, memoryTreebank0, stack0);
  }

  @Test(timeout = 4000)
  public void testTrainFromTreebankTaking3Arguments()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
      negraPennTreebankParserParams0.pw();
      edu.stanford.nlp.parser.lexparser.Options options0 = new edu.stanford.nlp.parser.lexparser.Options();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(options0, false, false);
      edu.stanford.nlp.parser.lexparser.Options.LexOptions options_LexOptions0 = new edu.stanford.nlp.parser.lexparser.Options.LexOptions();
      options0.lexOptions = options_LexOptions0;
      LinkedList<List<TaggedWord>> linkedList0 = new LinkedList<List<TaggedWord>>();
      Vector<CoreLabel> vector0 = new Vector<CoreLabel>();
      edu.stanford.nlp.trees.PennTreebankLanguagePack pennTreebankLanguagePack0 = new edu.stanford.nlp.trees.PennTreebankLanguagePack();
      edu.stanford.nlp.parser.lexparser.HebrewTreebankParserParams hebrewTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.HebrewTreebankParserParams(pennTreebankLanguagePack0);
      hebrewTreebankParserParams0.headFinder();
      edu.stanford.nlp.io.ExtensionFileFilter extensionFileFilter0 = new edu.stanford.nlp.io.ExtensionFileFilter("penn");
      LexicalizedParser.trainFromTreebank("stp", (FileFilter) extensionFileFilter0, options0);
      SemanticHeadFinder semanticHeadFinder0 = null;
      try {
        semanticHeadFinder0 = new SemanticHeadFinder();
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromTreebank()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
      MemoryTreebank memoryTreebank0 = negraPennTreebankParserParams0.memoryTreebank();
      negraPennTreebankParserParams0.pw();
      edu.stanford.nlp.parser.lexparser.Options options0 = new edu.stanford.nlp.parser.lexparser.Options();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(options0, false, false);
      edu.stanford.nlp.parser.lexparser.Options.LexOptions options_LexOptions0 = new edu.stanford.nlp.parser.lexparser.Options.LexOptions();
      options0.lexOptions = options_LexOptions0;
      options0.doPCFG = true;
      LinkedList<List<TaggedWord>> linkedList0 = new LinkedList<List<TaggedWord>>();
      // Undeclared exception!
      LexicalizedParser.getParserFromTreebank(memoryTreebank0, memoryTreebank0, (-10000.0), exactGrammarCompactor0, options0, memoryTreebank0, linkedList0);
  }

  @Test(timeout = 4000)
  public void testGetParserFromTreebankThrowsClassCastException()  throws Throwable  {
      EnglishTreebankParserParams englishTreebankParserParams0 = new EnglishTreebankParserParams();
      TreeReaderFactory treeReaderFactory0 = englishTreebankParserParams0.treeReaderFactory();
      DiskTreebank diskTreebank0 = new DiskTreebank(849, treeReaderFactory0);
      edu.stanford.nlp.parser.lexparser.Options options0 = new edu.stanford.nlp.parser.lexparser.Options(englishTreebankParserParams0);
      options0.doDep = false;
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(options0, false, false);
      SimpleConstituentFactory simpleConstituentFactory0 = new SimpleConstituentFactory();
      String string0 = "";
      Word word0 = new Word("");
      Constituent constituent0 = simpleConstituentFactory0.newConstituent(0, 0, (Label) word0, (-1980.979));
      simpleConstituentFactory0.newConstituent(2, 959);
      TreeGraphNode treeGraphNode0 = new TreeGraphNode(constituent0);
      Vector<List<TaggedWord>> vector0 = new Vector<List<TaggedWord>>();
      List<List<TaggedWord>> list0 = treeGraphNode0.yield(vector0);
      List<List<TaggedWord>> list1 = treeGraphNode0.yield(list0);
      // Undeclared exception!
      try { 
        LexicalizedParser.getParserFromTreebank(diskTreebank0, (edu.stanford.nlp.trees.Treebank) null, 0.0, exactGrammarCompactor0, options0, diskTreebank0, list1);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // class edu.stanford.nlp.ling.CoreLabel cannot be cast to class java.util.List (edu.stanford.nlp.ling.CoreLabel is in unnamed module of loader org.evosuite.instrumentation.InstrumentingClassLoader @510a8522; java.util.List is in module java.base of loader 'bootstrap')
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }

  @Test(timeout = 4000)
  public void testGetParserFromTreebankThrowsTooManyResourcesException()  throws Throwable  {
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
      MemoryTreebank memoryTreebank0 = negraPennTreebankParserParams0.memoryTreebank();
      edu.stanford.nlp.parser.lexparser.Options options0 = new edu.stanford.nlp.parser.lexparser.Options();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(options0, false, true);
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor1 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(shiftReduceOptions0, false, true);
      Stack<List<TaggedWord>> stack0 = new Stack<List<TaggedWord>>();
      // Undeclared exception!
      LexicalizedParser.getParserFromTreebank(memoryTreebank0, memoryTreebank0, (-1996.468229927831), exactGrammarCompactor0, shiftReduceOptions0, memoryTreebank0, stack0);
  }

  @Test(timeout = 4000)
  public void testTrainFromTreebankTaking3ArgumentsWithEmptyString()  throws Throwable  {
      edu.stanford.nlp.io.ExtensionFileFilter extensionFileFilter0 = new edu.stanford.nlp.io.ExtensionFileFilter("");
      edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams chineseTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams();
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      LexicalizedParser lexicalizedParser0 = LexicalizedParser.trainFromTreebank("", (FileFilter) extensionFileFilter0, (edu.stanford.nlp.parser.lexparser.Options) shiftReduceOptions0);
      assertFalse(lexicalizedParser0.requiresTags());
  }

  @Test(timeout = 4000)
  public void testGetParserFromTreebankThrowsNullPointerException()  throws Throwable  {
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer treeAnnotatorAndBinarizer0 = LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      LexicalizedParser.buildTrainTransformer((edu.stanford.nlp.parser.lexparser.Options) shiftReduceOptions0, treeAnnotatorAndBinarizer0);
      LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams negraPennTreebankParserParams0 = new edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams();
      MemoryTreebank memoryTreebank0 = negraPennTreebankParserParams0.memoryTreebank();
      edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor exactGrammarCompactor0 = new edu.stanford.nlp.parser.lexparser.ExactGrammarCompactor(shiftReduceOptions0, false, false);
      Stack<List<TaggedWord>> stack0 = new Stack<List<TaggedWord>>();
      // Undeclared exception!
      try { 
        LexicalizedParser.getParserFromTreebank((edu.stanford.nlp.trees.Treebank) null, memoryTreebank0, 3581.7166821414853, exactGrammarCompactor0, shiftReduceOptions0, memoryTreebank0, stack0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot invoke \"edu.stanford.nlp.trees.Treebank.transform(edu.stanford.nlp.trees.TreeTransformer)\" because \"trainTreebank\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer", e);
      }
  }

  @Test(timeout = 4000)
  public void testBuildTrainTransformerTaking2Arguments()  throws Throwable  {
      ShiftReduceOptions shiftReduceOptions0 = new ShiftReduceOptions();
      edu.stanford.nlp.parser.lexparser.TreeAnnotatorAndBinarizer treeAnnotatorAndBinarizer0 = LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      LexicalizedParser.buildTrainTransformer((edu.stanford.nlp.parser.lexparser.Options) shiftReduceOptions0, treeAnnotatorAndBinarizer0);
      LexicalizedParser.buildTrainBinarizer(shiftReduceOptions0);
      LexicalizedParser lexicalizedParser0 = LexicalizedParser.loadModel();
      assertNull(lexicalizedParser0);
  }

  @Test(timeout = 4000)
  public void testCopyLexicalizedParser()  throws Throwable  {
      // Undeclared exception!
      try { 
        LexicalizedParser.copyLexicalizedParser((LexicalizedParser) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // Cannot read field \"lex\" because \"parser\" is null
         //
         verifyException("edu.stanford.nlp.parser.lexparser.LexicalizedParser", e);
      }
  }
}
